# -*- coding: utf-8 -*-
"""cridit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B0c7h4uFkpWja-9ksPen0TtRDhOoYDMT
"""

# ========================================
# 1. INSTALL DEPENDENCIES (if required)
# ========================================
!pip install shap xgboost

# ========================================
# 2. IMPORT LIBRARIES
# ========================================
import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression

from xgboost import XGBClassifier
shap.utils._legacy = True   # Prevent deprecation warnings


# ========================================
# 3. LOAD DATASET
# ========================================
# Replace with your dataset file
df = pd.read_csv("credit_risk_dataset.csv")

target = "loan_default"
X = df.drop(target, axis=1)
y = df[target]


# ========================================
# 4. SPLIT NUMERIC & CATEGORICAL FEATURES
# ========================================
numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = X.select_dtypes(include=['object']).columns


# ========================================
# 5. PREPROCESSING PIPELINE
# ========================================
numeric_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent"))
])

preprocessor = ColumnTransformer([
    ("num", numeric_pipeline, numeric_cols),
    ("cat", categorical_pipeline, categorical_cols)
])


# ========================================
# 6. TRAIN / TEST SPLIT
# ========================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# ========================================
# 7. MODEL 1: LOGISTIC REGRESSION
# ========================================
log_model = Pipeline([
    ("prep", preprocessor),
    ("clf", LogisticRegression(max_iter=200))
])

log_model.fit(X_train, y_train)

y_pred_log = log_model.predict(X_test)

print("\n========== Logistic Regression Results ==========")
print(classification_report(y_test, y_pred_log))
print("Accuracy:", accuracy_score(y_test, y_pred_log))


# ========================================
# 8. MODEL 2: XGBOOST (Complex Model)
# ========================================
xgb_model = Pipeline([
    ("prep", preprocessor),
    ("clf", XGBClassifier(
            n_estimators=300,
            learning_rate=0.05,
            max_depth=5,
            subsample=0.8,
            colsample_bytree=0.8,
            eval_metric="logloss"
        ))
])

xgb_model.fit(X_train, y_train)

y_pred_xgb = xgb_model.predict(X_test)

print("\n========== XGBoost Model Results ==========")
print(classification_report(y_test, y_pred_xgb))
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))


# ========================================
# 9. SHAP EXPLAINABILITY
# ========================================

# Extract preprocessed training features
X_train_transformed = preprocessor.transform(X_train)
X_test_transformed = preprocessor.transform(X_test)


# ---- SHAP for Logistic Regression ----
log_explainer = shap.LinearExplainer(
    log_model.named_steps["clf"],
    X_train_transformed
)

log_shap_values = log_explainer(X_train_transformed)

# ---- SHAP for XGBoost ----
xgb_explainer = shap.TreeExplainer(
    xgb_model.named_steps["clf"]
)

xgb_shap_values = xgb_explainer(X_train_transformed)


# ========================================
# 10. SHAP GLOBAL SUMMARY PLOTS
# ========================================
print("\nGenerating SHAP Summary Plot for XGBoost...")
shap.summary_plot(xgb_shap_values, X_train_transformed, feature_names=list(numeric_cols) + list(categorical_cols))


# ========================================
# 11. SHAP FORCE PLOTS FOR 3 SAMPLE LOAN APPLICATIONS
# ========================================
indexes = [0, 50, 100]   # Change to choose different applicants

for i in indexes:
    print(f"\nSHAP Force Plot for Applicant #{i}")
    shap.force_plot(
        xgb_explainer.expected_value,
        xgb_shap_values[i],
        X_train.iloc[i],
        matplotlib=True
    )